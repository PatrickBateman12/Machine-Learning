{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77760965",
   "metadata": {},
   "source": [
    "### Cleaning and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings are provided to warn the developer of situations that arenâ€™t necessarily exceptions. \n",
    "#Usually, a warning occurs when there is some obsolete of certain programming elements.\n",
    "#Python program terminates immediately if an error occurs. Conversely, a warning is not critical.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd814b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('ConcatenatedResults/concatenated_results1.csv')\n",
    "\n",
    "validation_data = pd.read_csv('ConcatenatedResults/results_influx_validation_2023-03-27_12-48-45.csv')\n",
    "\n",
    "df_validation = validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b777a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with non-float64 data type\n",
    "data_original = data.select_dtypes(include=['float64', 'int64'])\n",
    "df_validation_original = df_validation.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "data = data_original\n",
    "df_validation = df_validation_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a7c8e",
   "metadata": {},
   "source": [
    "### Removing constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614dd57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_unique_columns(df):\n",
    "    for column in df.columns:\n",
    "        if column not in ['GasFlowRateOut', 'TotalInfluxMass'] and df[column].nunique() == 1:\n",
    "            df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "# Remove unique columns from df_train\n",
    "data = remove_unique_columns(data)\n",
    "\n",
    "# Remove unique columns from df_validation\n",
    "df_validation = remove_unique_columns(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86587adc",
   "metadata": {},
   "source": [
    "### Splitting each unique simulation into first 420 seconds into train, next 60 seconds into val and last 120 seconds into test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets based on unique simulation IDs\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "unique_sim_ids = data['sim_ID'].unique()\n",
    "\n",
    "for sim_id in unique_sim_ids:\n",
    "    sim_id_data = data[data['sim_ID'] == sim_id]\n",
    "\n",
    "    # Determine the split indices\n",
    "    split_index_train = int(len(sim_id_data) * 0.7)\n",
    "    split_index_val = split_index_train + int(len(sim_id_data) * 0.10)\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_data_sim = sim_id_data[:split_index_train]\n",
    "    val_data_sim = sim_id_data[split_index_train:split_index_val]\n",
    "    test_data_sim = sim_id_data[split_index_val:]\n",
    "\n",
    "    # Append the split data to the respective lists\n",
    "    train_data.append(train_data_sim)\n",
    "    val_data.append(val_data_sim)\n",
    "    test_data.append(test_data_sim)\n",
    "\n",
    "# Concatenate the dataframes in the lists to create a single dataframe for train, validation, and test\n",
    "df_train = pd.concat(train_data)\n",
    "df_val = pd.concat(val_data)\n",
    "df_test = pd.concat(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282d0ff",
   "metadata": {},
   "source": [
    "### Scaling using the MinMaxScaler before Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data separately for train, validation, and test sets\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler using the training set (excluding the 'sim_ID' column)\n",
    "scaler.fit(df_train.drop('sim_ID', axis=1))\n",
    "\n",
    "# Transform the data using the fitted scaler\n",
    "df_train_scaled = scaler.transform(df_train.drop('sim_ID', axis=1))\n",
    "df_val_scaled = scaler.transform(df_val.drop('sim_ID', axis=1))\n",
    "df_test_scaled = scaler.transform(df_test.drop('sim_ID', axis=1))\n",
    "\n",
    "# Convert the scaled arrays back to dataframes, preserving the column names (except for 'sim_ID')\n",
    "df_train_scaled = pd.DataFrame(df_train_scaled, columns=df_train.columns.drop('sim_ID'))\n",
    "df_val_scaled = pd.DataFrame(df_val_scaled, columns=df_val.columns.drop('sim_ID'))\n",
    "df_test_scaled = pd.DataFrame(df_test_scaled, columns=df_test.columns.drop('sim_ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75889e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap for MinMaxScaler\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title(\"Heatmap with MinMaxScaler\")\n",
    "sns.heatmap(df_train_scaled.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3354756b",
   "metadata": {},
   "source": [
    "### Removing features which are not relevant and are also printed in brackets below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Highly Correlated Features\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = df_train.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.9 or less than -0.9\n",
    "high_correlation = [column for column in upper.columns if any((upper[column] > 0.9) | (upper[column] < -0.9))]\n",
    "\n",
    "print(high_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03459e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_features = ['GasFlowRateOut','TotalInfluxMass', 'TopOfStringPosition', 'MainPitTemperature', 'ActivePitVolume', 'ActivePitTemperature', 'ActivePitDensity', 'CalculatedPressureBottomHole', 'DownholeECD', 'TopOfStringVelocity']\n",
    "\n",
    "# Combine the train, validation, and test sets\n",
    "combined_df = pd.concat([df_train, df_val, df_test])\n",
    "\n",
    "# Unscaled the combined data using inverse_transform\n",
    "unscaled_data = scaler.inverse_transform(combined_df.drop('sim_ID', axis=1))\n",
    "\n",
    "# Create a DataFrame with the unscaled data\n",
    "unscaled_df = pd.DataFrame(unscaled_data, columns=combined_df.columns.drop('sim_ID'))\n",
    "\n",
    "# Drop the unwanted features\n",
    "selected_features_df = unscaled_df.drop(columns=unwanted_features)\n",
    "\n",
    "# Scale the data back with the selected features\n",
    "minmax_scaled_data_selected = scaler.fit_transform(selected_features_df)\n",
    "\n",
    "# Create a DataFrame with the selected and scaled data\n",
    "minmax_scaled_df_selected = pd.DataFrame(minmax_scaled_data_selected, columns=selected_features_df.columns)\n",
    "\n",
    "# Split the data back into train, validation, and test sets\n",
    "train_len = len(df_train)\n",
    "val_len = len(df_val)\n",
    "\n",
    "df_train_selected = minmax_scaled_df_selected.iloc[:train_len]\n",
    "df_val_selected = minmax_scaled_df_selected.iloc[train_len:train_len+val_len]\n",
    "df_test_selected = minmax_scaled_df_selected.iloc[train_len+val_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e68c8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the heatmap for MinMaxScaler\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title(\"Heatmap with MinMaxScaler\")\n",
    "sns.heatmap(df_train_selected.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529815f",
   "metadata": {},
   "source": [
    "### Removing unwanted features and scaling the other dataframes aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation2 = df_validation.drop(columns=[col for col in unwanted_features])\n",
    "\n",
    "scaled_data_validation = scaler.fit_transform(df_validation2)\n",
    "\n",
    "scaled_data_validation = pd.DataFrame(scaled_data_validation, columns=df_validation2.columns)\n",
    "\n",
    "df_validation1 = scaled_data_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d88aca",
   "metadata": {},
   "source": [
    "### Storing the datasets, scaler and input/target column in the notebook for further ML in anoter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00a319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store the DataFrame in the IPython database\n",
    "%store df_train_selected\n",
    "%store df_val_selected\n",
    "%store df_test_selected\n",
    "%store df_validation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e9dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b12c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of column names to include in the new DataFrame\n",
    "input_col1 = [col for col in df_train_selected.columns if col != 'DownholePressure']\n",
    "\n",
    "target_col1 = ['DownholePressure']\n",
    "\n",
    "%store input_col1\n",
    "%store target_col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_train_selected into x_train and y_train\n",
    "x_train1 = df_train_selected[input_col1].to_numpy()\n",
    "y_train1 = df_train_selected[target_col1].to_numpy()\n",
    "\n",
    "# Split df_val_selected into x_val and y_val\n",
    "x_val1 = df_val_selected[input_col1].to_numpy()\n",
    "y_val1 = df_val_selected[target_col1].to_numpy()\n",
    "\n",
    "# Split df_test_selected into x_test and y_test\n",
    "x_test1 = df_test_selected[input_col1].to_numpy()\n",
    "y_test1 = df_test_selected[target_col1].to_numpy()\n",
    "\n",
    "# Store the arrays in the IPython database\n",
    "%store x_train1\n",
    "%store y_train1\n",
    "%store x_val1\n",
    "%store y_val1\n",
    "%store x_test1\n",
    "%store y_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_train into x_train and y_train\n",
    "x_train = df_train_selected[input_col1]\n",
    "y_train = df_train_selected[target_col1]\n",
    "\n",
    "# Split df_train into x_val and y_val\n",
    "x_val = df_val_selected[input_col1]\n",
    "y_val = df_val_selected[target_col1]\n",
    "\n",
    "# Split the testing data\n",
    "x_test = df_test_selected[input_col1]\n",
    "y_test = df_test_selected[target_col1]\n",
    "\n",
    "#Split the validation data\n",
    "x_validation = df_validation1[input_col1]\n",
    "y_validation = df_validation1[target_col1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddacdf",
   "metadata": {},
   "source": [
    "### Plots to see how the features look against timesteps for split method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_train_selected and df_test_selected are your train_data and test_data DataFrames\n",
    "df_train_plot = df_train_selected.head(420)\n",
    "df_val_plot = df_val_selected.head(59)\n",
    "df_test_plot = df_test_selected.head(119)\n",
    "\n",
    "fig, axs = plt.subplots(3, figsize=(15, 10)) # Create three subplots, adjust the figsize here (width, height)\n",
    "\n",
    "# Plot MainPitDensity and FlowRateOut for df_train_plot\n",
    "color1 = 'tab:blue'\n",
    "color2 = 'tab:red'\n",
    "axs[0].set_xlabel('Index')\n",
    "axs[0].set_ylabel('MainPitDensity', color=color1)\n",
    "axs[0].plot(df_train_plot.index, df_train_plot['MainPitDensity'].fillna(0), color=color1)\n",
    "axs[0].tick_params(axis='y', labelcolor=color1)\n",
    "axs[0].set_title('MainPitDensity and FlowRateOut for First 420 Rows (Training Data)')\n",
    "\n",
    "ax2 = axs[0].twinx()\n",
    "ax2.set_ylabel('FlowRateOut', color=color2)\n",
    "ax2.plot(df_train_plot.index, df_train_plot['FlowRateOut'], color=color2)\n",
    "ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Plot MainPitDensity and FlowRateOut for df_val_plot\n",
    "axs[1].set_xlabel('Index')\n",
    "axs[1].set_ylabel('MainPitDensity', color=color1)\n",
    "axs[1].plot(df_val_plot.index, df_val_plot['MainPitDensity'].fillna(0), color=color1)\n",
    "axs[1].tick_params(axis='y', labelcolor=color1)\n",
    "axs[1].set_title('MainPitDensity and FlowRateOut for Next 60 Rows (Validation Data)')\n",
    "\n",
    "ax3 = axs[1].twinx()\n",
    "ax3.set_ylabel('FlowRateOut', color=color2)\n",
    "ax3.plot(df_val_plot.index, df_val_plot['FlowRateOut'], color=color2)\n",
    "ax3.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Plot MainPitDensity and FlowRateOut for df_test_plot\n",
    "axs[2].set_xlabel('Index')\n",
    "axs[2].set_ylabel('MainPitDensity', color=color1)\n",
    "axs[2].plot(df_test_plot.index, df_test_plot['MainPitDensity'].fillna(0), color=color1)\n",
    "axs[2].tick_params(axis='y', labelcolor=color1)\n",
    "axs[2].set_title('MainPitDensity and FlowRateOut for Next 120 Rows (Test Data)')\n",
    "\n",
    "ax4 = axs[2].twinx()\n",
    "ax4.set_ylabel('FlowRateOut', color=color2)\n",
    "ax4.plot(df_test_plot.index, df_test_plot['FlowRateOut'], color=color2)\n",
    "ax4.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524795c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "features = [\n",
    "    \"MainPitDensity\",\n",
    "    \"FlowRateOut\",\n",
    "    \"SPP\",\n",
    "    \"InstantaneousROP\",\n",
    "    \"FlowRateIn\",\n",
    "    \"FluidTemperatureOut\",\n",
    "    \"MainPitVolume\",\n",
    "    \"HookLoad\",\n",
    "    \"SurfaceTorque\",\n",
    "    \"BitDepth\",\n",
    "    \"timeStep\", \n",
    "    \"DownholePressure\"\n",
    "]\n",
    "\n",
    "# Assuming df_train is your training DataFrame\n",
    "plot_data = df_train_selected.head(420)\n",
    "\n",
    "fig, axs = plt.subplots(len(features), figsize=(15, 30), sharex=True)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axs[i].plot(plot_data[feature], label=feature)\n",
    "    axs[i].set_ylabel(feature)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
