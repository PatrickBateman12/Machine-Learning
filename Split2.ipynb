{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "644ef161",
   "metadata": {},
   "source": [
    "### Cleaning and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261c5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings are provided to warn the developer of situations that arenâ€™t necessarily exceptions. \n",
    "#Usually, a warning occurs when there is some obsolete of certain programming elements.\n",
    "#Python program terminates immediately if an error occurs. Conversely, a warning is not critical.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd814b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "kick_data = pd.read_csv('ConcatenatedResults/concatenated_results_kick.csv')\n",
    "\n",
    "normal_data = pd.read_csv('ConcatenatedResults/concatenated_results_normal.csv')\n",
    "\n",
    "validation_data = pd.read_csv('ConcatenatedResults/results_influx_validation_2023-03-27_12-48-45.csv')\n",
    "\n",
    "df_validation = validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d51f52",
   "metadata": {},
   "source": [
    "### Splitting the data by Method 2, 70/10/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_sim_id(df, test_size=0.2, val_size=0.1, random_state=42):\n",
    "   sim_ids = df['sim_ID'].unique()\n",
    "   train_sim_ids, test_val_sim_ids = train_test_split(sim_ids, test_size=test_size+val_size, random_state=random_state)\n",
    "   test_sim_ids, val_sim_ids = train_test_split(test_val_sim_ids, test_size=test_size/(test_size+val_size), random_state=random_state)\n",
    "\n",
    "   train_data = df[df['sim_ID'].isin(train_sim_ids)]\n",
    "   val_data = df[df['sim_ID'].isin(val_sim_ids)]\n",
    "   test_data = df[df['sim_ID'].isin(test_sim_ids)]\n",
    "\n",
    "   return train_data, val_data, test_data\n",
    "\n",
    "kick_train, kick_val, kick_test = split_data_by_sim_id(kick_data)\n",
    "normal_train, normal_val, normal_test = split_data_by_sim_id(normal_data)\n",
    "\n",
    "#Concatenate the resulting train, validation, and test sets\n",
    "df_train = pd.concat([kick_train, normal_train], ignore_index=True)\n",
    "df_val = pd.concat([kick_val, normal_val], ignore_index=True)\n",
    "df_test = pd.concat([kick_test, normal_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b018aae2",
   "metadata": {},
   "source": [
    "### Removing constant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with non-float64 data type\n",
    "df_train_original = df_train.select_dtypes(include=['float64', 'int64'])\n",
    "df_val_original = df_val.select_dtypes(include=['float64', 'int64'])\n",
    "df_test_original = df_test.select_dtypes(include=['float64', 'int64'])\n",
    "df_validation_original = df_validation.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "df_train = df_train_original.drop(['sim_ID'], axis=1)\n",
    "df_val = df_val_original.drop(['sim_ID'], axis=1)\n",
    "df_test = df_test_original.drop(['sim_ID'], axis=1)\n",
    "df_validation = df_validation_original.drop(['sim_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614dd57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remove_unique_columns(df):\n",
    "    for column in df.columns:\n",
    "        if column not in ['GasFlowRateOut', 'TotalInfluxMass'] and df[column].nunique() == 1:\n",
    "            df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "# Remove unique columns from df_train\n",
    "df_train = remove_unique_columns(df_train)\n",
    "\n",
    "df_val = remove_unique_columns(df_val)\n",
    "\n",
    "df_test = remove_unique_columns(df_test)\n",
    "\n",
    "# Remove unique columns from df_validation\n",
    "df_validation = remove_unique_columns(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b13c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4f9bc",
   "metadata": {},
   "source": [
    "### Scaling using the MinMaxScaler before Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883233f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the scalers\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using both scalers\n",
    "minmax_scaled_data = minmax_scaler.fit_transform(df_train)\n",
    "\n",
    "# Create DataFrames with the scaled data\n",
    "minmax_scaled_df = pd.DataFrame(minmax_scaled_data, columns=df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75889e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap for MinMaxScaler\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title(\"Heatmap with MinMaxScaler\")\n",
    "sns.heatmap(minmax_scaled_df.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbaa30e",
   "metadata": {},
   "source": [
    "### Removing features which are not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03459e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'unwanted_features' is a list of column names you want to drop after feature selection\n",
    "unwanted_features = ['GasFlowRateOut', 'TotalInfluxMass','TopOfStringPosition', 'MainPitTemperature', 'ActivePitVolume', 'ActivePitTemperature', 'ActivePitDensity', 'CalculatedPressureBottomHole', 'DownholeECD', 'TopOfStringVelocity']\n",
    "\n",
    "# Unscaled the data using inverse_transform\n",
    "unscaled_data = minmax_scaler.inverse_transform(minmax_scaled_df)\n",
    "\n",
    "# Create a DataFrame with the unscaled data\n",
    "unscaled_df = pd.DataFrame(unscaled_data, columns=df_train.columns)\n",
    "\n",
    "# Drop the unwanted features\n",
    "selected_features_df = unscaled_df.drop(columns=unwanted_features)\n",
    "\n",
    "minmax_scaled_data_selected = minmax_scaler.fit_transform(selected_features_df)\n",
    "\n",
    "# Create a DataFrame with the selected and scaled data\n",
    "minmax_scaled_df_selected = pd.DataFrame(minmax_scaled_data_selected, columns=selected_features_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e68c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap for MinMaxScaler\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.title(\"Heatmap with MinMaxScaler\")\n",
    "sns.heatmap(minmax_scaled_df_selected.corr(), annot=True, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aaea57",
   "metadata": {},
   "source": [
    "### Removing unwanted features and scaling the other dataframes aswell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c26b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = minmax_scaled_df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vali = df_validation.drop(columns=unwanted_features)\n",
    "\n",
    "scaled_data_validation = minmax_scaler.fit_transform(df_vali)\n",
    "\n",
    "scaled_data_validation = pd.DataFrame(scaled_data_validation, columns=df_vali.columns)\n",
    "\n",
    "df_validation2 = scaled_data_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6a204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unwanted columns from df_test, excluding the 'TopOfStringVelocity' column\n",
    "df_test1 = df_test.drop(columns=[col for col in unwanted_features])\n",
    "\n",
    "# Scale the remaining columns using MinMaxScaler\n",
    "scaled_data_test = minmax_scaler.fit_transform(df_test1)\n",
    "\n",
    "# Create a new dataframe from the scaled data and use the column names from df_test1\n",
    "scaled_data_test = pd.DataFrame(scaled_data_test, columns=df_test1.columns)\n",
    "\n",
    "# Assign the new dataframe to df_test\n",
    "df_test2 = scaled_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb59c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unwanted columns from df_test, excluding the 'TopOfStringVelocity' column\n",
    "df_val1 = df_val.drop(columns=[col for col in unwanted_features])\n",
    "\n",
    "# Scale the remaining columns using MinMaxScaler\n",
    "scaled_data_val = minmax_scaler.fit_transform(df_val1)\n",
    "\n",
    "# Create a new dataframe from the scaled data and use the column names from df_test1\n",
    "scaled_data_val = pd.DataFrame(scaled_data_val, columns=df_val1.columns)\n",
    "\n",
    "# Assign the new dataframe to df_test\n",
    "df_val2 = scaled_data_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e8c19",
   "metadata": {},
   "source": [
    "### Storing the datasets, scaler and input/target column in the notebook for further ML in anoter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00a319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store the DataFrame in the IPython database\n",
    "%store df_train2\n",
    "%store df_val2\n",
    "%store df_test2\n",
    "%store df_validation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b12c0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a list of column names to include in the new DataFrame\n",
    "input_col2 = [col for col in df_train2.columns if col != 'DownholePressure']\n",
    "\n",
    "target_col2 = ['DownholePressure']\n",
    "\n",
    "%store input_col2\n",
    "%store target_col2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455c0aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minmax_scaler2 = minmax_scaler\n",
    "%store minmax_scaler2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a75823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_train into x_train and y_train\n",
    "x_train = df_train2[input_col2]\n",
    "y_train = df_train2[target_col2]\n",
    "\n",
    "x_val = df_val2[input_col2]\n",
    "y_val = df_val2[target_col2]\n",
    "\n",
    "# Split the testing data\n",
    "x_test = df_test2[input_col2]\n",
    "y_test = df_test2[target_col2]\n",
    "\n",
    "#Split the validation data\n",
    "x_validation = df_validation2[input_col2]\n",
    "y_validation = df_validation2[target_col2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d62c0e",
   "metadata": {},
   "source": [
    "### Plots to see how the features look against timesteps for split method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_train is your train_data DataFrame\n",
    "df_plot = df_train.head(600)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(15, 5))  # Adjust the figsize here (width, height)\n",
    "\n",
    "# Plot MainPitDensity with a blue color\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Index')\n",
    "ax1.set_ylabel('MainPitDensity', color=color)\n",
    "ax1.plot(df_plot.index, df_plot['MainPitDensity'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "# Create a second y-axis to plot FlowRateOut with a red color\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('FlowRateOut', color=color)\n",
    "ax2.plot(df_plot.index, df_plot['FlowRateOut'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.title('MainPitDensity and FlowRateOut for First 600 Rows')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524795c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"MainPitDensity\",\n",
    "    \"FlowRateOut\",\n",
    "    \"SPP\",\n",
    "    \"InstantaneousROP\",\n",
    "    \"FlowRateIn\",\n",
    "    \"FluidTemperatureOut\",\n",
    "    \"MainPitVolume\",\n",
    "    \"HookLoad\",\n",
    "    \"SurfaceTorque\",\n",
    "    \"BitDepth\",\n",
    "    \"timeStep\", \n",
    "    \"DownholePressure\"\n",
    "]\n",
    "\n",
    "# Assuming df_train is your training DataFrame\n",
    "plot_data = df_train.head(600)\n",
    "\n",
    "fig, axs = plt.subplots(len(features), figsize=(15, 30), sharex=True)\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axs[i].plot(plot_data[feature], label=feature)\n",
    "    axs[i].set_ylabel(feature)\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
