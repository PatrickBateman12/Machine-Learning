{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21d23c6",
   "metadata": {},
   "source": [
    "### Import the neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings are provided to warn the developer of situations that arenâ€™t necessarily exceptions. \n",
    "#Usually, a warning occurs when there is some obsolete of certain programming elements.\n",
    "#Python program terminates immediately if an error occurs. Conversely, a warning is not critical.\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f531c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb0aeeb",
   "metadata": {},
   "source": [
    "### Import data from Feature Selection Split 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4965b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the DataFrame in the IPython database\n",
    "%store -r df_train_selected\n",
    "%store -r df_val_selected\n",
    "%store -r df_test_selected\n",
    "%store -r df_validation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scaler\n",
    "%store -r input_col1\n",
    "input_col = input_col1\n",
    "%store -r target_col1\n",
    "target_col = target_col1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bb67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_train into x_train and y_train\n",
    "x_train = df_train_selected[input_col1]\n",
    "y_train = df_train_selected[target_col1]\n",
    "\n",
    "# Split df_train into x_val and y_val\n",
    "x_val = df_val_selected[input_col1]\n",
    "y_val = df_val_selected[target_col1]\n",
    "\n",
    "# Split the testing data\n",
    "x_test = df_test_selected[input_col1]\n",
    "y_test = df_test_selected[target_col1]\n",
    "\n",
    "#Split the validation data\n",
    "x_validation = df_validation1[input_col1]\n",
    "y_validation = df_validation1[target_col1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25fa11",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ac617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Initialize the model with the hyperparameters you want\n",
    "rf = RandomForestRegressor(max_depth=None, \n",
    "                           max_features='sqrt', \n",
    "                           min_samples_leaf=4, \n",
    "                           min_samples_split=10, \n",
    "                           n_estimators=200,\n",
    "                           random_state=42)\n",
    "\n",
    "# Fit the model on your training data\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# Initialize RFE, change n_features_to__select depending on how many features you want to rank\n",
    "rfe = RFE(estimator=rf, n_features_to_select=5, step=1)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(x_train, y_train)\n",
    "\n",
    "# Get the ranking of the features. The lower the rank, the better the feature.\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Get the feature importance\n",
    "importance = rf.feature_importances_\n",
    "\n",
    "# Get the feature names\n",
    "feature_names = x_train.columns\n",
    "\n",
    "# Create a DataFrame that combines feature names, ranks and importances\n",
    "feature_rank_importance = pd.DataFrame({'Feature': feature_names, 'Rank': ranking, 'Importance': importance})\n",
    "\n",
    "# Print the five most important features\n",
    "print(feature_rank_importance.sort_values('Rank').head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c8ee1",
   "metadata": {},
   "source": [
    "### Sobol and Morris Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb751323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SALib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from SALib.sample import saltelli, morris as morris_sample\n",
    "from SALib.analyze import sobol, morris\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assume x_train and y_train are already defined\n",
    "\n",
    "# Train a RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Define the problem for SALib\n",
    "problem = {\n",
    "    'num_vars': len(x_train.columns),\n",
    "    'names': x_train.columns.tolist(),\n",
    "    'bounds': np.column_stack((x_train.min().values, x_train.max().values)).tolist()\n",
    "}\n",
    "\n",
    "sobol_samples = saltelli.sample(problem, 1000)  # Adjust the second parameter for more/less samples\n",
    "\n",
    "# Generate samples for Morris SA\n",
    "morris_samples = morris_sample.sample(problem, 1000, num_levels=4)  # Removed 'grid_jump_fraction'\n",
    "\n",
    "# Evaluate the samples using the RandomForestRegressor model\n",
    "sobol_y = model.predict(sobol_samples)\n",
    "\n",
    "# Rest of the code\n",
    "morris_y = model.predict(morris_samples)\n",
    "\n",
    "# Perform Sobol SA\n",
    "sobol_analysis = sobol.analyze(problem, sobol_y, print_to_console=True)\n",
    "\n",
    "# Perform Morris SA\n",
    "morris_analysis = morris.analyze(problem, morris_samples, morris_y, print_to_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db26391f",
   "metadata": {},
   "source": [
    "### Grid Search for Random Forest, Extra Gradient Booster and ADABoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the hyperparameter grid for the random forest regressor\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, None],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grid for the XGBoost regressor\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'subsample': [0.5, 0.8, 1],\n",
    "    'colsample_bytree': [0.5, 0.8, 1]\n",
    "}\n",
    "\n",
    "# Define the hyperparameter grid for the AdaBoost regressor\n",
    "ada_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1],\n",
    "    'loss': ['linear', 'square', 'exponential']\n",
    "}\n",
    "\n",
    "# Define the models to be tuned\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "ada = AdaBoostRegressor(random_state=42)\n",
    "\n",
    "# Define the parameter grids to be used for each model\n",
    "param_grids = [rf_param_grid, xgb_param_grid, ada_param_grid]\n",
    "\n",
    "# Define the models to be tuned and their corresponding parameter grids\n",
    "models = [(rf, rf_param_grid), (xgb, xgb_param_grid), (ada, ada_param_grid)]\n",
    "\n",
    "# Loop over each model and its corresponding parameter grid, and perform a grid search\n",
    "for model, param_grid in models:\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(f\"Best hyperparameters for {type(model).__name__}:\", grid_search.best_params_)\n",
    "    \n",
    "    # Predict on the validation set using the best model\n",
    "    val_preds = grid_search.best_estimator_.predict(x_val)\n",
    "\n",
    "    # Calculate the mean squared error on the validation set\n",
    "    val_mse = mean_squared_error(y_val, val_preds)\n",
    "    print(f'Validation MSE for {type(model).__name__}:', val_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
